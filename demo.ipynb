{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import torch\n",
    "\n",
    "CUDA_VISIBLE_DEVICES = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_VISIBLE_DEVICES\n",
    "\n",
    "gender_info = {\n",
    "    \"P1\": 0, \"P3\": 0, \"P4\": 0, \"P7\": 0, \"P8\": 0, \"P10\": 0, \"P13\": 0, \"P15\": 0,\n",
    "    \"P2\": 1, \"P5\": 1, \"P6\": 1, \"P9\": 1, \"P11\": 1, \"P12\": 1, \"P14\": 1, \"P16\": 1,\n",
    "    \"P17\": 1, \"P18\": 1, \"P19\": 1, \"P20\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e475fe",
   "metadata": {},
   "source": [
    "# Loading A small Demo dataset with RGB, Depth, RPC, RT and GT Mesh modalities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7c51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded Successfully from ../cached_data_test_vis/rf3dpose_all ...\n",
      "Kept 5000 / 5000 samples after filtering.\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset_mmMesh2_vis import RF3DPoseDataset, ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset_vis = RF3DPoseDataset([], transform=ToTensor(), load_save=True, use_image=True, is_demo=True, cache_dir=\"../cached_data_test_vis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1fe395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 frames starting from index 0 for combined GIF with 4 columns...\n",
      "tensor([  2,   1, 100]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 101]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 102]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 103]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 104]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 105]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 106]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 107]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 108]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 109]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 110]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 111]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 112]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 113]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 114]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 115]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 116]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 117]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 118]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 119]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 120]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 121]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 122]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 123]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 124]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 125]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 126]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 127]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 128]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   1, 129]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "Saved combined GIF with 2x3 grid: vis_depth/combined_output_depth_1.gif\n",
      "Generating 5000 frames starting from index 0 for combined GIF with 4 columns...\n",
      "tensor([  2,   2, 100]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 101]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 102]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 103]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 104]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 105]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 106]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 107]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 108]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 109]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 110]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 111]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 112]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 113]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 114]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 115]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 116]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 117]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 118]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 119]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 120]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 121]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 122]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 123]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 124]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 125]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 126]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 127]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 128]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   2, 129]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "Saved combined GIF with 2x3 grid: vis_depth/combined_output_depth_2.gif\n",
      "Generating 5000 frames starting from index 0 for combined GIF with 4 columns...\n",
      "tensor([  2,   3, 100]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 101]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 102]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 103]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 104]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 105]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 106]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 107]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 108]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 109]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 110]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 111]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 112]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 113]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 114]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 115]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 116]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 117]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 118]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 119]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 120]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 121]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 122]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 123]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 124]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 125]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 126]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 127]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 128]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n",
      "tensor([  2,   3, 129]) image: torch.Size([3, 480, 640]), depth: torch.Size([1, 480, 640]), radar_tensor: torch.Size([121, 111, 31]), radar_points: torch.Size([1000, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader_Plotting_projection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_frames_for_gif_depth\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\u001b[38;5;66;03m#50\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mplot_frames_for_gif_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_vis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_gif_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvis_depth/combined_output_depth_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.gif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mmwave_human/M4Human/MR-Mesh-main1/dataset/data_loader_Plotting_projection.py:682\u001b[0m, in \u001b[0;36mplot_frames_for_gif_depth\u001b[0;34m(dataset, start_idx, max_frames, combined_gif_path, act_id, is_plot)\u001b[0m\n\u001b[1;32m    680\u001b[0m         combined_frames\u001b[38;5;241m.\u001b[39mappend(grid_frame)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_plot\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(combined_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_gif_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved combined GIF with 2x3 grid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_gif_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_plot\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(combined_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Display the combined frames as an animation\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/v2.py:362\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/core/legacy_plugin_wrapper.py:253\u001b[0m, in \u001b[0;36mLegacyPlugin.write\u001b[0;34m(self, ndimage, is_batch, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(image\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mnumber) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\n\u001b[1;32m    247\u001b[0m             image\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    248\u001b[0m         ):\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll images have to be numeric, and not `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[0;32m--> 253\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m writer\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/core/format.py:590\u001b[0m, in \u001b[0;36mFormat.Writer.append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    588\u001b[0m im \u001b[38;5;241m=\u001b[39m asarray(im)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/plugins/pillowmulti.py:84\u001b[0m, in \u001b[0;36mGIFFormat.Writer._append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m     82\u001b[0m     duration \u001b[38;5;241m=\u001b[39m duration[\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(duration) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39m_count)]\n\u001b[1;32m     83\u001b[0m dispose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispose\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/plugins/pillowmulti.py:128\u001b[0m, in \u001b[0;36mGifWriter.add_image\u001b[0;34m(self, im, duration, dispose)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_subrectangle:\n\u001b[1;32m    127\u001b[0m     im_rect, rect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetSubRectangle(im)\n\u001b[0;32m--> 128\u001b[0m im_pil \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverToPIL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_rect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_palette_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Get pallette - apparently, this is the 3d element of the header\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# (but it has not always been). Best we've got. Its not the same\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# as im_pil.palette.tobytes().\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGifImagePlugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getheader\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/imageio/plugins/pillowmulti.py:329\u001b[0m, in \u001b[0;36mGifWriter.converToPIL\u001b[0;34m(self, im, quantizer, palette_size)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m         im_pil \u001b[38;5;241m=\u001b[39m im_pil\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m     im_pil \u001b[38;5;241m=\u001b[39m \u001b[43mim_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpalette_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for quantizer: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m quantizer)\n",
      "File \u001b[0;32m~/.conda/envs/p4_mamba1/lib/python3.9/site-packages/PIL/Image.py:1131\u001b[0m, in \u001b[0;36mImage.quantize\u001b[0;34m(self, colors, method, kmeans, palette, dither)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     new_im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m palette\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_im\n\u001b[0;32m-> 1131\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImagePalette\n\u001b[1;32m   1135\u001b[0m mode \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mgetpalettemode()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset.data_loader_Plotting_projection import plot_frames_for_gif_depth\n",
    "for i in range(10):#50\n",
    "    plot_frames_for_gif_depth(dataset_vis, combined_gif_path=f'vis_depth/combined_output_depth_{i+1}.gif', act_id=i+1, is_plot=\"save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb4a5f",
   "metadata": {},
   "source": [
    "# Loading Full dataset with only radar-based RPC, RT and GT Mesh modalities..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07287dd0",
   "metadata": {},
   "source": [
    "### dataset param:\n",
    "\n",
    "1. Split: train, test, val\n",
    "\n",
    "2. main_modality: selected modality: default rt (No need to change, both provided)\n",
    "\n",
    "3. protocol_id: (controlling the size of dataset)\n",
    "\n",
    "Here is different from protocol in paper [IP, SIP, NIP], these protocol performance will be printed during main training. Can adjust to accomodate your expected training time!!\n",
    "\n",
    "p1: 100% dataset, p2: 50% dataset, p3: 25% dataset\n",
    "\n",
    "4. split_id: (controlling split)\n",
    "\n",
    "Same as paper. s1: random split; s2: cross-subject split; s3: cross-action split\n",
    "\n",
    "5. Temporal_window: sliding window size for concat adjacent radar frame.\n",
    "\n",
    "Default: 4\n",
    "\n",
    "6. Load_save: loading flag for preprocessed dataset. Better set to True, haven't test False can process from scratch or not.\n",
    "\n",
    "7. Cache_dir: saved preprocessed dataset path. [***!!! input your own dir !!!***]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7257578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded Successfully from ../mmDataset/MR-Mesh/rf3dpose_all ...\n",
      "Load indices from pre-saved ../mmDataset/MR-Mesh/rf3dpose_all/indeces.pkl.gz indices splits.\n",
      "Loaded train Dataset with length 495512.\n",
      "Unique sub in train: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Unique act in train: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Kept 495506 / 495512 samples after filtering.\n",
      "Dataset loaded Successfully from ../mmDataset/MR-Mesh/rf3dpose_all ...\n",
      "Load indices from pre-saved ../mmDataset/MR-Mesh/rf3dpose_all/indeces.pkl.gz indices splits.\n",
      "Loaded test Dataset with length 131809.\n",
      "Unique sub in test: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Unique act in test: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Kept 131806 / 131809 samples after filtering.\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset_mmMesh2 import RF3DPoseDataset, ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset_train = RF3DPoseDataset([], \n",
    "                                # Train-Test Specifics\n",
    "                                split = \"train\",  \n",
    "                                main_modality=\"rt\", \n",
    "                                protocol_id=\"p1\", \n",
    "                                split_id=\"s1\", \n",
    "                                temporal_window=6,\n",
    "                                # meta info\n",
    "                                load_save=True, \n",
    "                                cache_dir=\"../mmDataset/MR-Mesh/\",  \n",
    "                                transform=transforms.Compose([ToTensor()]), \n",
    "                                )\n",
    "dataset_test = RF3DPoseDataset([], \n",
    "                               # Train-Test Specifics\n",
    "                               split = \"test\",  \n",
    "                               main_modality=\"rt\", \n",
    "                               protocol_id=\"p1\", \n",
    "                               split_id=\"s1\", \n",
    "                               temporal_window=6,\n",
    "                               # meta info\n",
    "                               load_save=True, \n",
    "                               cache_dir=\"../mmDataset/MR-Mesh/\",  \n",
    "                               transform=transforms.Compose([ToTensor()]), \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a013d4",
   "metadata": {},
   "source": [
    "### Dataloader getitem: \n",
    "Dict of components consist of Radar Modality[RT / RPC], Sample ID, GT information [Param, Calibration, bounding box, 2D projected mesh in RGB frame], RGBD modality (TODO)\n",
    "\n",
    "### Details:\n",
    "Sample ID: [indicator] [Sub_id, Act_id, Frame_id];\n",
    "\n",
    "Radar RT: [rawImage_XYZ], (T_window, Azim, Depth, Elev), T_window: default 4;\n",
    "\n",
    "Radar RPC: [radar_points], (T_window, N, 4), N = 1000 with zero-padding;\n",
    "\n",
    "GT mesh: [vertices]; GT bbbox: [bbbox]; GT param: [parameter]; \n",
    "\n",
    "GT calib: [calibration]; GT tracking loc: [joint_root]: coord of tracking joint 0. \n",
    "\n",
    "GT 2D mesh proj: [projected_vertices] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e1294de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: tensor([ 1, 10,  1])\n",
      "Sample keys: dict_keys(['rawImage_XYZ', 'vertices', 'bbbox', 'projected_vertices', 'parameter', 'calibration', 'indicator', 'radar_points', 'joints_root'])\n",
      "\n",
      "== Sample Content ==\n",
      "\n",
      "--- Key 1/9 ---\n",
      "rawImage_XYZ: shape torch.Size([6, 121, 111, 31]), dtype torch.float32\n",
      "--- Key 2/9 ---\n",
      "vertices: shape torch.Size([10475, 3]), dtype torch.float32\n",
      "--- Key 3/9 ---\n",
      "bbbox->pc: type <class 'dict'>\n",
      "bbbox->tensor: type <class 'dict'>\n",
      "--- Key 4/9 ---\n",
      "projected_vertices: shape torch.Size([10475, 2]), dtype torch.float32\n",
      "--- Key 5/9 ---\n",
      "parameter->joints: shape torch.Size([55, 3]), dtype torch.float32\n",
      "parameter->betas: shape torch.Size([10]), dtype torch.float32\n",
      "parameter->pose_body: shape torch.Size([63]), dtype torch.float32\n",
      "parameter->trans: shape torch.Size([3]), dtype torch.float32\n",
      "parameter->root_orient: shape torch.Size([3]), dtype torch.float32\n",
      "parameter->gender: shape torch.Size([1]), dtype torch.float32\n",
      "--- Key 6/9 ---\n",
      "calibration->cam_intrinsic: shape torch.Size([3, 3]), dtype torch.float32\n",
      "calibration->vicon_to_cam_rotmatrix: shape torch.Size([3, 3]), dtype torch.float32\n",
      "calibration->vicon_to_cam_tvec: shape torch.Size([3]), dtype torch.float32\n",
      "calibration->radar_to_cam_rotmatrix: shape torch.Size([3, 3]), dtype torch.float32\n",
      "calibration->radar_to_cam_tvec: shape torch.Size([3]), dtype torch.float32\n",
      "--- Key 7/9 ---\n",
      "indicator: shape torch.Size([3]), dtype torch.int64\n",
      "--- Key 8/9 ---\n",
      "radar_points: shape torch.Size([6, 1000, 4]), dtype torch.float32\n",
      "--- Key 9/9 ---\n",
      "joints_root: shape torch.Size([22, 3]), dtype torch.float32\n"
     ]
    }
   ],
   "source": [
    "sample = dataset_train[0]\n",
    "print(\"Sample ID:\", sample.get('indicator', 'N/A'))\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"\\n== Sample Content ==\\n\")\n",
    "\n",
    "for i,key in zip(range(len(sample)), sample):\n",
    "    print(f\"--- Key {i+1}/{len(sample)} ---\")\n",
    "    if isinstance(sample[key], torch.Tensor):\n",
    "        print(f\"{key}: shape {sample[key].shape}, dtype {sample[key].dtype}\")\n",
    "    elif isinstance(sample[key], dict):\n",
    "        for subkey in sample[key]:\n",
    "            if isinstance(sample[key][subkey], torch.Tensor):\n",
    "                print(f\"{key}->{subkey}: shape {sample[key][subkey].shape}, dtype {sample[key][subkey].dtype}\")\n",
    "            else:\n",
    "                print(f\"{key}->{subkey}: type {type(sample[key][subkey])}\")\n",
    "    else:\n",
    "        print(f\"{key}: type {type(sample[key])}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p4_mamba1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
